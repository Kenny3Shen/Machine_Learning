{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kenny3s/lfw-transformer?scriptVersionId=157677262\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"8896de97","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-04T13:04:37.900606Z","iopub.status.busy":"2024-01-04T13:04:37.900224Z","iopub.status.idle":"2024-01-04T13:05:49.059185Z","shell.execute_reply":"2024-01-04T13:05:49.05822Z"},"papermill":{"duration":71.166349,"end_time":"2024-01-04T13:05:49.061814","exception":false,"start_time":"2024-01-04T13:04:37.895465","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/CLIP.git\r\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-snwt8eeu\r\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-snwt8eeu\r\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\r\n","  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n","\u001b[?25hCollecting ftfy (from clip==1.0)\r\n","  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/91/f8/dfa32d06cfcbdb76bc46e0f5d69c537de33f4cedb1a15cd4746ab45a6a26/ftfy-6.1.3-py3-none-any.whl.metadata\r\n","  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\r\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.8.8)\r\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.1)\r\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\r\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\r\n","Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->clip==1.0)\r\n","  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/31/b1/a59de0ad3aabb17523a39804f4c6df3ae87ead053a4e25362ae03d73d03a/wcwidth-0.2.12-py2.py3-none-any.whl.metadata\r\n","  Downloading wcwidth-0.2.12-py2.py3-none-any.whl.metadata (14 kB)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.2)\r\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\r\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\r\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.24.3)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\r\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.1.0)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.2.0)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\r\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\r\n","Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\r\n","Building wheels for collected packages: clip\r\n","  Building wheel for clip (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=d50497e565fcff3be4aa07ed0996c2c93d0bbaf2a6b0ed1931eadc1d53bbc048\r\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hvm6qghp/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\r\n","Successfully built clip\r\n","Installing collected packages: wcwidth, ftfy, clip\r\n","  Attempting uninstall: wcwidth\r\n","    Found existing installation: wcwidth 0.2.6\r\n","    Uninstalling wcwidth-0.2.6:\r\n","      Successfully uninstalled wcwidth-0.2.6\r\n","Successfully installed clip-1.0 ftfy-6.1.3 wcwidth-0.2.12\r\n","Collecting open_clip_torch\r\n","  Obtaining dependency information for open_clip_torch from https://files.pythonhosted.org/packages/7c/7f/952fdffa17b15d0c7c51a730860fcf4f4982528ecc753b190dcd46cc944b/open_clip_torch-2.23.0-py3-none-any.whl.metadata\r\n","  Downloading open_clip_torch-2.23.0-py3-none-any.whl.metadata (30 kB)\r\n","Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2.0.0)\r\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.15.1)\r\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2023.8.8)\r\n","Requirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (6.1.3)\r\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (4.66.1)\r\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.17.3)\r\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.1.99)\r\n","Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (3.20.3)\r\n","Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.9.10)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.12.2)\r\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.5.0)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.12)\r\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\r\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.12)\r\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2023.10.0)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.31.0)\r\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.1)\r\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (21.3)\r\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->open_clip_torch) (0.4.0)\r\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.24.3)\r\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (10.1.0)\r\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.0.9)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.3)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.2.0)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.15)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2023.7.22)\r\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\r\n","Downloading open_clip_torch-2.23.0-py3-none-any.whl (1.5 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: open_clip_torch\r\n","Successfully installed open_clip_torch-2.23.0\r\n","Collecting sentence_transformers\r\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.35.0)\r\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\r\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\r\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\r\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.24.3)\r\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\r\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.3)\r\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\r\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\r\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.17.3)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\r\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.10.0)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\r\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\r\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\r\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\r\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\r\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\r\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.14.1)\r\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\r\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\r\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\r\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\r\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.1.0)\r\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\r\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\r\n","Building wheels for collected packages: sentence_transformers\r\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n","\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=b82abed5ed206f191bd2eb48fc4e9d508ad0a73c828cabc7369f3b58effaa3cc\r\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\r\n","Successfully built sentence_transformers\r\n","Installing collected packages: sentence_transformers\r\n","Successfully installed sentence_transformers-2.2.2\r\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","100%|████████████████████████████████████████| 834M/834M [00:08<00:00, 102MiB/s]\n"]}],"source":["!pip install git+https://github.com/openai/CLIP.git\n","!pip install open_clip_torch\n","!pip install sentence_transformers\n","\n","import torch\n","import open_clip\n","import cv2\n","from sentence_transformers import util\n","import torchvision.datasets as datasets\n","from PIL import Image\n","import numpy as np\n","import os\n","# image processing model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n","model.to(device)\n","def imageEncoder(img):\n","    img1 = Image.fromarray(img).convert('RGB')\n","    img1 = preprocess(img1).unsqueeze(0).to(device)\n","    img1 = model.encode_image(img1)\n","    return img1\n","def generateScore(image1, image2):\n","    test_img = cv2.imread(image1, cv2.IMREAD_UNCHANGED)\n","    data_img = cv2.imread(image2, cv2.IMREAD_UNCHANGED)\n","    img1 = imageEncoder(test_img)\n","    img2 = imageEncoder(data_img)\n","    cos_scores = util.pytorch_cos_sim(img1, img2)\n","    score = round(float(cos_scores[0][0])*100, 2)\n","    return score\n","#print(f\"similarity Score: \", round(generateScore(image1, image2), 2))\n"]},{"cell_type":"code","execution_count":2,"id":"0fc8eb13","metadata":{"execution":{"iopub.execute_input":"2024-01-04T13:05:49.094507Z","iopub.status.busy":"2024-01-04T13:05:49.094172Z","iopub.status.idle":"2024-01-04T13:05:49.109413Z","shell.execute_reply":"2024-01-04T13:05:49.108516Z"},"papermill":{"duration":0.0339,"end_time":"2024-01-04T13:05:49.111365","exception":false,"start_time":"2024-01-04T13:05:49.077465","status":"completed"},"tags":[]},"outputs":[],"source":["class LFWDataset(datasets.ImageFolder):\n","    def __init__(self, dir, pairs_path, image_size, transform=None, half_face=False, both=False):\n","        super(LFWDataset, self).__init__(dir, transform)\n","        self.image_size = image_size\n","        self.pairs_path = pairs_path\n","        self.validation_images = self.get_lfw_paths(dir)\n","        self.half = half_face\n","        self.both = both\n","\n","    def read_lfw_pairs(self, pairs_filename):\n","        pairs = []\n","        with open(pairs_filename, 'r') as f:\n","            for line in f.readlines()[1:]:\n","                pair = line.strip().split()\n","                pairs.append(pair)\n","        return np.array(pairs, dtype=object)\n","\n","    def get_lfw_paths(self, lfw_dir, file_ext=\"png\"):\n","\n","        pairs = self.read_lfw_pairs(self.pairs_path)\n","\n","        nrof_skipped_pairs = 0\n","        path_list = []\n","        issame_list = []\n","\n","        for i in range(len(pairs)):\n","            # for pair in pairs:\n","            pair = pairs[i]\n","            if len(pair) == 3:\n","                path0 = os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[1]) + '.' + file_ext)\n","                path1 = os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[2]) + '.' + file_ext)\n","                issame = True\n","            elif len(pair) == 4:\n","                path0 = os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[1]) + '.' + file_ext)\n","                path1 = os.path.join(lfw_dir, pair[2], pair[2] + '_' + '%04d' % int(pair[3]) + '.' + file_ext)\n","                issame = False\n","            if os.path.exists(path0) and os.path.exists(path1):  # Only add the pair if both paths exist\n","                path_list.append((path0, path1, issame))\n","                issame_list.append(issame)\n","            else:\n","                nrof_skipped_pairs += 1\n","        if nrof_skipped_pairs > 0:\n","            print('Skipped %d image pairs' % nrof_skipped_pairs)\n","\n","        return path_list\n","\n","#     def __getitem__(self, index):\n","#         (path_1, path_2, issame) = self.validation_images[index]\n","#         image1, image2 = Image.open(path_1), Image.open(path_2)\n","\n","#         if self.half:\n","#             image2 = image2.convert('1')\n","#             if self.both:\n","#                 image1 = image1.convert('1')\n","\n","#         image1 = resize_image(image1, [self.image_size[1], self.image_size[0]], letterbox_image=True)\n","#         image2 = resize_image(image2, [self.image_size[1], self.image_size[0]], letterbox_image=True)\n","\n","#         image1, image2 = np.transpose(preprocess_input(np.array(image1, np.float32)), [2, 0, 1]), np.transpose(\n","#             preprocess_input(np.array(image2, np.float32)), [2, 0, 1])\n","\n","#         return image1, image2, issame\n","    def __getitem__(self, index):\n","        (path_1, path_2, issame) = self.validation_images[index]\n","        return path_1, path_2, issame\n","\n","    def __len__(self):\n","        return len(self.validation_images)"]},{"cell_type":"code","execution_count":3,"id":"db645700","metadata":{"execution":{"iopub.execute_input":"2024-01-04T13:05:49.138768Z","iopub.status.busy":"2024-01-04T13:05:49.138417Z","iopub.status.idle":"2024-01-04T13:06:08.936578Z","shell.execute_reply":"2024-01-04T13:06:08.935552Z"},"papermill":{"duration":19.814507,"end_time":"2024-01-04T13:06:08.938995","exception":false,"start_time":"2024-01-04T13:05:49.124488","status":"completed"},"tags":[]},"outputs":[],"source":["dataset = LFWDataset(dir=\"/kaggle/input/lfw-codeformer/lfw_codeformer\", pairs_path=\"/kaggle/input/lfwpeople/pairs.txt\", image_size=(512, 512))"]},{"cell_type":"code","execution_count":4,"id":"2d72c75a","metadata":{"execution":{"iopub.execute_input":"2024-01-04T13:06:08.967717Z","iopub.status.busy":"2024-01-04T13:06:08.967064Z","iopub.status.idle":"2024-01-04T13:12:07.928291Z","shell.execute_reply":"2024-01-04T13:12:07.927224Z"},"papermill":{"duration":358.97811,"end_time":"2024-01-04T13:12:07.930727","exception":false,"start_time":"2024-01-04T13:06:08.952617","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 6000/6000 [05:54<00:00, 16.93it/s]\n","100%|██████████| 101/101 [00:04<00:00, 22.06it/s]"]},{"name":"stdout","output_type":"stream","text":["Best threshold = 58\n","Accuracy = 0.8318333333333333\n","Precision = 0.8224813735017816\n","Recall = 0.8463333333333334\n","F1_score = 0.8342368983078692\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from tqdm import tqdm, trange\n","# Step 1: Calculate scores for all image pairs\n","scores = []\n","labels = []\n","for i in trange(len(dataset)):\n","    image1_path, image2_path, issame = dataset[i]\n","    score = generateScore(image1_path, image2_path)\n","    scores.append(score)\n","    labels.append(issame)\n","\n","# Step 2: Calculate metrics for a range of thresholds\n","best_accuracy = 0\n","best_thresh = 0\n","best_precision = 0\n","best_recall = 0\n","best_f1 = 0\n","for thresh in tqdm(np.arange(0, 101)):  # assuming scores are in range 0-100\n","    predictions = [s > thresh for s in scores]\n","    accuracy = accuracy_score(labels, predictions)\n","    precision = precision_score(labels, predictions, zero_division=1)\n","    recall = recall_score(labels, predictions, zero_division=1)\n","    f1 = f1_score(labels, predictions, zero_division=1)\n","    if accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_thresh = thresh\n","        best_precision = precision\n","        best_recall = recall\n","        best_f1 = f1\n","\n","# Step 3: Print the best threshold and corresponding metrics\n","print(f\"Best threshold = {best_thresh}\")\n","print(f\"Accuracy = {best_accuracy}\")\n","print(f\"Precision = {best_precision}\")\n","print(f\"Recall = {best_recall}\")\n","print(f\"F1_score = {best_f1}\")"]},{"cell_type":"code","execution_count":null,"id":"28ad8726","metadata":{"papermill":{"duration":0.231458,"end_time":"2024-01-04T13:12:08.395514","exception":false,"start_time":"2024-01-04T13:12:08.164056","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"1333ea75","metadata":{"papermill":{"duration":0.229162,"end_time":"2024-01-04T13:12:08.855778","exception":false,"start_time":"2024-01-04T13:12:08.626616","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":26922,"sourceId":34595,"sourceType":"datasetVersion"},{"datasetId":19136,"sourceId":796646,"sourceType":"datasetVersion"},{"datasetId":4042928,"sourceId":7029158,"sourceType":"datasetVersion"},{"datasetId":4202404,"sourceId":7252830,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":456.497507,"end_time":"2024-01-04T13:12:11.111611","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-04T13:04:34.614104","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}