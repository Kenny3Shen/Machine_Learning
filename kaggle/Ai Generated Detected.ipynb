{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67046cbe-73c1-4216-9813-a38f02153757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/lizhecheng/balanced-weights-longer-training-higher-score\n",
    "# https://www.kaggle.com/code/bhanupratapbiswas/llm-detect-ai-generated-text-02\n",
    "# 抄这两篇然后根据自己的理解改了下，只有数据处理的部分, 还没搭模型\n",
    "# 有空可以看一下这两篇开头写的\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import language_tool_python\n",
    "\n",
    "from transformers import To\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ef6887c-da74-4cd6-bb99-ae79511c196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_data = pd.read_csv(\"train_essays.csv\")\n",
    "test_data = pd.read_csv(\"test_essays.csv\")\n",
    "train_data = pd.concat([daigt_data, essay_data])\n",
    "prompts = pd.read_csv(\"train_prompts.csv\")\n",
    "\n",
    "# https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset/data\n",
    "# https://www.kaggle.com/datasets/nbroad/persaude-corpus-2/\n",
    "# daigt data 上面两网址有描述 有兴趣可以看 可能是因为官方提供的数据比较少 所以得用外部的数据来训练模型\n",
    "daigt_data = pd.read_csv(\"train_v2_drcat_02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2d28888-978b-405a-a558-010edad49473",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tool = language_tool_python.LanguageTool(\"en-US\")  # 一个语法检查器\n",
    "# pip install language-tool-python\n",
    "# 机子要有java才能下载工具包 没有java或者不想装也可以手动下载 https://languagetool.org/download/LanguageTool-stable.zip\n",
    "# 解压后放到~/.cache/language_tool_python/\n",
    "\n",
    "\n",
    "# 想了解可以看 https://pypi.org/project/language-tool-python/\n",
    "# 用语法检查器修改句子\n",
    "def correct_sentence(sentence):\n",
    "    return language_tool.correct(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a30b8d45-f754-46fd-92cf-f2445416d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_persuade_data = daigt_data[\n",
    "    daigt_data[\"source\"] != \"persuade_corpus\"\n",
    "]  # 直译：不具有说服性的的文章数据\n",
    "# 我的理解是这些所谓的not_persuade_data应该是指不那么正式的文章，可能就是人与人之间交流的句子，然后一些网络上的夹着表情的话\n",
    "# 建议自己拿一篇出来看一下，或者也可以翻译成非议论文\n",
    "persuade_data = daigt_data[\n",
    "    daigt_data[\"source\"] == \"persuade_corpus\"\n",
    "]  # 直译：具有说服性的文章数据 或者议论文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0a063f8-579c-476f-a03f-149f54ce7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_characters = set(list(\"\".join(persuade_data.text.to_list())))  # 具有说服性的文章里所用到的字符\n",
    "all_characters = set(\n",
    "    list(\"\".join(not_persuade_data.text.to_list()))\n",
    ")  # 不具有说服性的文章里所用到的字符\n",
    "remove_list = \"\".join(all_characters - formal_characters)  # 两个集合拿差集 得到要除去的字符表\n",
    "\n",
    "translation_table = str.maketrans(\"\", \"\", remove_list)  # 把得到的字符表塞入这个函数，通过remove_chars函数\n",
    "\n",
    "\n",
    "# 每个字符串出现这种字符就会删掉\n",
    "def remove_chars(string):\n",
    "    return string.translate(translation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ace0a800-804a-4def-8974-d5cc9e0c85c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'📷💰🌲💻🌎🐠📅🍋🏊🍷🙀💘🚫🌊💊🏳护🎉”️驶🎃有🏜🎣🌄选安🇷ê😷…😝🏖🤓🔜🥨🎠🤤禁😢🍁部🗳🧠🥛🚨唯🙌🏰😴😉📱🚑🤖😊🐒🌫🐆🗣ã🔥こり💦せр手🚴🏋🧀💇ん🤘🧘😤💸🥁🛣🧐💼🏙🦸道🥤💀😍\\r😂🛍😩‘💃🧡🐕🏛🏨🍴’з–🚌🐾🐴🥩🥯🛀🕰🤫🦄🇺中📖🔋🤯🌧🏠和😠É😲都👏必😮🧚🎩🎧🌱🐳👥🏔🍞🛫🤕🥘👕👍🍝意🎥🇸响み的司🎭😔🧑�🥲🎯👻🥟👬🍮😘😹🐟💔法🎨と😋🍲👋😃🎸😭♂🌟🚂🤝🎬🎓🔬🏄🌽🏼🌳□🌮🎄须🚗🏈🌐🌃🧭🧹👮🥔者д🎅择😒す🌞п😆🤟🛠😬🍎📰🤜🎤😵💅集📞あ时🚔👨🐶🍟🥖🧦🤪所🛋🏦╯👩全🏢🍖🦁🌴👧🔮с该🛸🥭💭注🍄📺🛬\\u200d😄💧👇🕹🌨🎵🚀🐝💚🕵🌅🥑📹📊🏯💥🧙🌿🥜🐸ü🚣🥶😖👯💖🎊🏡📸🚕一取🏽💬保─应🌯机🙋🇧🙈🐢🤣💉😕📉🇯😎ā💨🍜🚚ç🐱🤛力に🔧🎮а用е🌷🤢🧩🥗🌻🐭止🥳🦐😓📄🍰😁\\u200b❄🎶驾🚭☹🔭—💜🏫🏻将🌸🔑路上🌭🧬🤩🧖🧽🍳が😻😱📧💕😡是🇵🍕📚🐬☀う🍣📈🤗合😌✨💡🛑🏏🙅在🥕🐰🙊🤷👀😅о😳🌈🦎🍓💆♀⚽😈🐦🐻💪🏕🌌💫🤔👂📦📝🎾👌🥪🏃🇪🍗🌠🇫í🙃🏀🌏👦😨🐧👪ま🍭は💤🍿÷🚪🥦😜⏰影🍽使🍔ち💯💁🏟🏞🙏🕒🎹📣🙄“🤦🏥🔍🎢完🤒🏆。🤞🕺🎈👫'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_list  # 要移除的字符表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c2dddee-29b8-499d-ae78-e143dce9b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"text\"] = train_data[\"text\"].apply(remove_chars)  # 移除非正式字符\n",
    "train_data[\"text\"] = train_data[\"text\"].str.replace(\"\\n\", \"\")  # \\n 换成 ''\n",
    "\n",
    "test_data[\"text\"] = test_data[\"text\"].str.replace(\"\\n\", \"\")\n",
    "test_data[\"text\"] = test_data[\"text\"].apply(remove_chars)  # 移除非正式字符\n",
    "test_data[\"text\"] = test_data[\"text\"].apply(correct_sentence)  # 用语法检查器改句子\n",
    "\n",
    "data = pd.concat([train_data[\"text\"], test_data[\"text\"]], axis=0)  # 训练集和测试集合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7575fd96-e85d-4b46-badd-f3f0a725beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把句子向量化 提取特征\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "# 正则表达式：r'[^\\W]+' 直接看例子\n",
    "# \"This is an example sentence with various punctuation and whitespace.\"\n",
    "# 应用正则表达式后\n",
    "# ['This', 'is', 'an', 'example', 'sentence', 'with', 'various', 'punctuation', 'and', 'whitespace']\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    tokenizer=lambda x: re.findall(r\"[^\\W]+\", x),\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "vectorizer = vectorizer.fit(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c48579c-6662-4f05-a13f-4577da77a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85b69df2-cd49-4bd4-bb3d-70e900f7c3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e8166-0eb5-4883-a80c-cd4718cc2659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
